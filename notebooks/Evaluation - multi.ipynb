{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of simulations\n",
    "\n",
    "Generate figures based on simulated experiments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from analysis import Evaluation, MultipleEvaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the directories `OUTPUT_DIR` holding the simulation data, and `PLOT_DIR` to save the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '../temp_pickles/'\n",
    "PLOT_DIR = '../plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the directory holding all experiments of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir_name = 'reprod_refactoredCode'\n",
    "# exp_dir_name = 'reprod_workshop_plots'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all plots for each experiments, as well as a plot comparing all-settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the summary for multi-setting comparison\n",
    "multi_summary = collections.defaultdict(dict)\n",
    "\n",
    "# Analyze each selected experiment\n",
    "for fpath in glob.glob(os.path.join(OUTPUT_DIR, exp_dir_name, '*.pkl')):\n",
    "    print(f\"=== Analyzing experiment: {fpath:s}\")\n",
    "    # Extract experiment name\n",
    "    expname = os.path.split(os.path.splitext(fpath)[0])[1]\n",
    "    # Load data from pickle file\n",
    "    data = joblib.load(fpath)\n",
    "\n",
    "    policy_list = [d['name'] for d in data]\n",
    "    dat = [d['dat'] for d in data]\n",
    "    n_trials = len(dat[0])\n",
    "\n",
    "    print(\"Make plots of the experiments:\")\n",
    "    evaluation = Evaluation(\n",
    "        data=dat, \n",
    "        plot_dir=os.path.join(PLOT_DIR, exp_dir_name, expname), \n",
    "        description=policy_list)\n",
    "\n",
    "#         print('- make present_discounted_loss...')\n",
    "#         evaluation.present_discounted_loss(plot=True, save=True)\n",
    "\n",
    "    print('- make simulation_plot(X)...')\n",
    "    evaluation.simulation_plot(\n",
    "        process='X', filename='simulation_infection_summary',\n",
    "        granularity=0.1, save=True)\n",
    "\n",
    "    print('- make simulation_plot(H)...')\n",
    "    evaluation.simulation_plot(\n",
    "        process='H', filename='simulation_treatment_summary',\n",
    "        granularity=0.1, save=True)\n",
    "\n",
    "    print('- make simulation_plot(u)...')\n",
    "    evaluation.simulation_plot(\n",
    "        process='u', filename='simulation_control_rate_summary',\n",
    "        granularity=0.1, save=True)\n",
    "\n",
    "    # Compute Comparison analysis data\n",
    "    print(\"Compute comparison analysis data:\")\n",
    "    summary_tup = evaluation.infections_and_interventions_complete(size_tup=(16, 10), save=True)\n",
    "    multi_summary['infections_and_interventions'][expname] = summary_tup\n",
    "    multi_summary['Qs'][expname] = evaluation.data[0][0]['info']['Qx'][0]\n",
    "\n",
    "# Save data for multi-setting comparison for future reuse (small disk space)\n",
    "joblib.dump(value={'multi_summary': multi_summary, 'policy_list': policy_list, 'n_trials': n_trials}, \n",
    "            filename=os.path.join(PLOT_DIR, exp_dir_name, 'multi_comp_dump.pkl'))\n",
    "\n",
    "# Comparative analysis\n",
    "multi_eval = MultipleEvaluations(\n",
    "    multi_summary, policy_list, n_trials, save_dir=os.path.join(PLOT_DIR, exp_dir_name)\n",
    ")\n",
    "multi_eval.compare_infections();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
